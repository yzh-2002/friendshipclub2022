{"version":3,"sources":["index.js","locales/en.js","ls.js","lib/entry-index.js","lib/content/path.js","package.json","lib/util/hash-to-segments.js","lib/util/fix-owner.js","lib/util/y.js","get.js","lib/memoization.js","lib/content/read.js","put.js","lib/content/write.js","lib/util/move-file.js","rm.js","lib/content/rm.js","verify.js","lib/verify.js","lib/util/tmp.js"],"names":[],"mappings":";;;;;;;AAAA;AACA;AACA;AACA,ACHA;AACA;AACA;AACA,ACHA;ADIA,ACHA;ADIA,ACHA;ACFA,AFMA,ACHA;ACFA,AFMA,ACHA;ACFA,AFMA,ACHA;AELA,ADGA,AFMA,ACHA;AELA,ADGA,AFMA;AGRA,ADGA,AFMA;AGRA,ADGA,AFMA,AIZA;ADIA,ADGA,AFMA,AIZA;ADIA,ADGA,AFMA,AIZA;ADIA,ADGA,AGTA,ALeA,AIZA;ADIA,ADGA,AGTA,ALeA,AIZA;ADIA,ADGA,AGTA,ALeA,AIZA;ADIA,ADGA,AIZA,ADGA,ALeA,AIZA;ADIA,ADGA,AIZA,ADGA,ALeA,AIZA;ADIA,ADGA,AIZA,ADGA,ALeA,AIZA;ADIA,ADGA,AIZA,ADGA,AENA,APqBA,AIZA;ADIA,ADGA,AIZA,ADGA,AENA,APqBA,AIZA;ADIA,ADGA,AIZA,ADGA,AENA,APqBA,AIZA;AIXA,ALeA,ADGA,AIZA,ADGA,AENA,APqBA,AIZA;AIXA,ALeA,ADGA,AIZA,ADGA,AENA,APqBA,AIZA;AIXA,ALeA,ADGA,AIZA,ADGA,AENA,APqBA,AIZA;AIXA,ALeA,ADGA,AOrBA,AHSA,ACHA,APqBA,AIZA;AIXA,ALeA,ADGA,AOrBA,AHSA,ACHA,APqBA,AIZA;AIXA,ALeA,ADGA,AOrBA,AHSA,ACHA,APqBA,AIZA;AIXA,ALeA,AOrBA,ARwBA,AOrBA,AHSA,ACHA,APqBA,AIZA;AIXA,ALeA,AOrBA,ARwBA,AOrBA,AHSA,ACHA,APqBA,AIZA;AIXA,ALeA,AOrBA,ARwBA,AOrBA,AHSA,ACHA,APqBA,AIZA;AIXA,ALeA,AOrBA,ARwBA,AOrBA,AHSA,ACHA,APqBA,AIZA,AOrBA;AHUA,ALeA,AOrBA,ARwBA,AOrBA,AHSA,ACHA,APqBA,AIZA,AOrBA;AHUA,ALeA,AOrBA,ARwBA,AOrBA,AHSA,ACHA,APqBA,AIZA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,ACHA,APqBA,AIZA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,ACHA,APqBA,AIZA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,ACHA,APqBA,AIZA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,ANkBA,APqBA,AIZA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,ANkBA,APqBA,AIZA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,ANkBA,APqBA,AIZA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,ANkBA,APqBA,AIZA,AOrBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,ANkBA,APqBA,AIZA,AOrBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,ANkBA,APqBA,AIZA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,ANkBA,APqBA,AIZA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,ANkBA,APqBA,AIZA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AbuCA,AIZA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AT2BA,AOrBA,AGTA,AENA;ARyBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AT2BA,AOrBA,AGTA,AENA;ARyBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AT2BA,AOrBA,AGTA,AENA;ARyBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AIZA,AbuCA,AOrBA,AGTA,AENA;ARyBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AIZA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AIZA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AOrBA,AKfA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AYpCA,ADGA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AOrBA,AHSA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA,AOrBA;AHUA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA,AbuCA;AIXA,AENA,AENA,AV8BA,AIZA,AWjCA;AT4BA,AENA,AENA,AV8BA,AIZA,AWjCA;AT4BA,AENA,AENA,AV8BA,AIZA,AWjCA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;AT4BA,ANkBA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"index.js","sourcesContent":["\n\nmodule.exports = require('./locales/en.js')\n","\n\nconst ls = require('../ls.js')\nconst get = require('../get.js')\nconst put = require('../put.js')\nconst rm = require('../rm.js')\nconst verify = require('../verify.js')\nconst setLocale = require('../lib/util/y.js').setLocale\nconst clearMemoized = require('../lib/memoization.js').clearMemoized\nconst tmp = require('../lib/util/tmp.js')\n\nsetLocale('en')\n\nconst x = module.exports\n\nx.ls = cache => ls(cache)\nx.ls.stream = cache => ls.stream(cache)\n\nx.get = (cache, key, opts) => get(cache, key, opts)\nx.get.byDigest = (cache, hash, opts) => get.byDigest(cache, hash, opts)\nx.get.sync = (cache, key, opts) => get.sync(cache, key, opts)\nx.get.sync.byDigest = (cache, key, opts) => get.sync.byDigest(cache, key, opts)\nx.get.stream = (cache, key, opts) => get.stream(cache, key, opts)\nx.get.stream.byDigest = (cache, hash, opts) => get.stream.byDigest(cache, hash, opts)\nx.get.copy = (cache, key, dest, opts) => get.copy(cache, key, dest, opts)\nx.get.copy.byDigest = (cache, hash, dest, opts) => get.copy.byDigest(cache, hash, dest, opts)\nx.get.info = (cache, key) => get.info(cache, key)\nx.get.hasContent = (cache, hash) => get.hasContent(cache, hash)\nx.get.hasContent.sync = (cache, hash) => get.hasContent.sync(cache, hash)\n\nx.put = (cache, key, data, opts) => put(cache, key, data, opts)\nx.put.stream = (cache, key, opts) => put.stream(cache, key, opts)\n\nx.rm = (cache, key) => rm.entry(cache, key)\nx.rm.all = cache => rm.all(cache)\nx.rm.entry = x.rm\nx.rm.content = (cache, hash) => rm.content(cache, hash)\n\nx.setLocale = lang => setLocale(lang)\nx.clearMemoized = () => clearMemoized()\n\nx.tmp = {}\nx.tmp.mkdir = (cache, opts) => tmp.mkdir(cache, opts)\nx.tmp.withTmp = (cache, opts, cb) => tmp.withTmp(cache, opts, cb)\n\nx.verify = (cache, opts) => verify(cache, opts)\nx.verify.lastRun = cache => verify.lastRun(cache)\n","\n\nvar index = require('./lib/entry-index')\n\nmodule.exports = index.ls\nmodule.exports.stream = index.lsStream\n","\n\nconst BB = require('bluebird')\n\nconst contentPath = require('./content/path')\nconst crypto = require('crypto')\nconst figgyPudding = require('figgy-pudding')\nconst fixOwner = require('./util/fix-owner')\nconst fs = require('graceful-fs')\nconst hashToSegments = require('./util/hash-to-segments')\nconst ms = require('mississippi')\nconst path = require('path')\nconst ssri = require('ssri')\nconst Y = require('./util/y.js')\n\nconst indexV = require('../package.json')['cache-version'].index\n\nconst appendFileAsync = BB.promisify(fs.appendFile)\nconst readFileAsync = BB.promisify(fs.readFile)\nconst readdirAsync = BB.promisify(fs.readdir)\nconst concat = ms.concat\nconst from = ms.from\n\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor (cache, key) {\n    super(Y`No cache entry for \\`${key}\\` found in \\`${cache}\\``)\n    this.code = 'ENOENT'\n    this.cache = cache\n    this.key = key\n  }\n}\n\nconst IndexOpts = figgyPudding({\n  metadata: {},\n  size: {}\n})\n\nmodule.exports.insert = insert\nfunction insert (cache, key, integrity, opts) {\n  opts = IndexOpts(opts)\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size: opts.size,\n    metadata: opts.metadata\n  }\n  return fixOwner.mkdirfix(\n    cache, path.dirname(bucket)\n  ).then(() => {\n    const stringified = JSON.stringify(entry)\n    // NOTE - Cleverness ahoy!\n    //\n    // This works because it's tremendously unlikely for an entry to corrupt\n    // another while still preserving the string length of the JSON in\n    // question. So, we just slap the length in there and verify it on read.\n    //\n    // Thanks to @isaacs for the whiteboarding session that ended up with this.\n    return appendFileAsync(\n      bucket, `\\n${hashEntry(stringified)}\\t${stringified}`\n    )\n  }).then(\n    () => fixOwner.chownr(cache, bucket)\n  ).catch({ code: 'ENOENT' }, () => {\n    // There's a class of race conditions that happen when things get deleted\n    // during fixOwner, or between the two mkdirfix/chownr calls.\n    //\n    // It's perfectly fine to just not bother in those cases and lie\n    // that the index entry was written. Because it's a cache.\n  }).then(() => {\n    return formatEntry(cache, entry)\n  })\n}\n\nmodule.exports.insert.sync = insertSync\nfunction insertSync (cache, key, integrity, opts) {\n  opts = IndexOpts(opts)\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size: opts.size,\n    metadata: opts.metadata\n  }\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket))\n  const stringified = JSON.stringify(entry)\n  fs.appendFileSync(\n    bucket, `\\n${hashEntry(stringified)}\\t${stringified}`\n  )\n  try {\n    fixOwner.chownr.sync(cache, bucket)\n  } catch (err) {\n    if (err.code !== 'ENOENT') {\n      throw err\n    }\n  }\n  return formatEntry(cache, entry)\n}\n\nmodule.exports.find = find\nfunction find (cache, key) {\n  const bucket = bucketPath(cache, key)\n  return bucketEntries(bucket).then(entries => {\n    return entries.reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next)\n      } else {\n        return latest\n      }\n    }, null)\n  }).catch(err => {\n    if (err.code === 'ENOENT') {\n      return null\n    } else {\n      throw err\n    }\n  })\n}\n\nmodule.exports.find.sync = findSync\nfunction findSync (cache, key) {\n  const bucket = bucketPath(cache, key)\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next)\n      } else {\n        return latest\n      }\n    }, null)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null\n    } else {\n      throw err\n    }\n  }\n}\n\nmodule.exports.delete = del\nfunction del (cache, key, opts) {\n  return insert(cache, key, null, opts)\n}\n\nmodule.exports.delete.sync = delSync\nfunction delSync (cache, key, opts) {\n  return insertSync(cache, key, null, opts)\n}\n\nmodule.exports.lsStream = lsStream\nfunction lsStream (cache) {\n  const indexDir = bucketDir(cache)\n  const stream = from.obj()\n\n  // \"/cachename/*\"\n  readdirOrEmpty(indexDir).map(bucket => {\n    const bucketPath = path.join(indexDir, bucket)\n\n    // \"/cachename/<bucket 0xFF>/*\"\n    return readdirOrEmpty(bucketPath).map(subbucket => {\n      const subbucketPath = path.join(bucketPath, subbucket)\n\n      // \"/cachename/<bucket 0xFF>/<bucket 0xFF>/*\"\n      return readdirOrEmpty(subbucketPath).map(entry => {\n        const getKeyToEntry = bucketEntries(\n          path.join(subbucketPath, entry)\n        ).reduce((acc, entry) => {\n          acc.set(entry.key, entry)\n          return acc\n        }, new Map())\n\n        return getKeyToEntry.then(reduced => {\n          for (let entry of reduced.values()) {\n            const formatted = formatEntry(cache, entry)\n            formatted && stream.push(formatted)\n          }\n        }).catch({ code: 'ENOENT' }, nop)\n      })\n    })\n  }).then(() => {\n    stream.push(null)\n  }, err => {\n    stream.emit('error', err)\n  })\n\n  return stream\n}\n\nmodule.exports.ls = ls\nfunction ls (cache) {\n  return BB.fromNode(cb => {\n    lsStream(cache).on('error', cb).pipe(concat(entries => {\n      cb(null, entries.reduce((acc, xs) => {\n        acc[xs.key] = xs\n        return acc\n      }, {}))\n    }))\n  })\n}\n\nfunction bucketEntries (bucket, filter) {\n  return readFileAsync(\n    bucket, 'utf8'\n  ).then(data => _bucketEntries(data, filter))\n}\n\nfunction bucketEntriesSync (bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8')\n  return _bucketEntries(data, filter)\n}\n\nfunction _bucketEntries (data, filter) {\n  let entries = []\n  data.split('\\n').forEach(entry => {\n    if (!entry) { return }\n    const pieces = entry.split('\\t')\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return\n    }\n    let obj\n    try {\n      obj = JSON.parse(pieces[1])\n    } catch (e) {\n      // Entry is corrupted!\n      return\n    }\n    if (obj) {\n      entries.push(obj)\n    }\n  })\n  return entries\n}\n\nmodule.exports._bucketDir = bucketDir\nfunction bucketDir (cache) {\n  return path.join(cache, `index-v${indexV}`)\n}\n\nmodule.exports._bucketPath = bucketPath\nfunction bucketPath (cache, key) {\n  const hashed = hashKey(key)\n  return path.join.apply(path, [bucketDir(cache)].concat(\n    hashToSegments(hashed)\n  ))\n}\n\nmodule.exports._hashKey = hashKey\nfunction hashKey (key) {\n  return hash(key, 'sha256')\n}\n\nmodule.exports._hashEntry = hashEntry\nfunction hashEntry (str) {\n  return hash(str, 'sha1')\n}\n\nfunction hash (str, digest) {\n  return crypto\n    .createHash(digest)\n    .update(str)\n    .digest('hex')\n}\n\nfunction formatEntry (cache, entry) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity) { return null }\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: contentPath(cache, entry.integrity),\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata\n  }\n}\n\nfunction readdirOrEmpty (dir) {\n  return readdirAsync(dir)\n    .catch({ code: 'ENOENT' }, () => [])\n    .catch({ code: 'ENOTDIR' }, () => [])\n}\n\nfunction nop () {\n}\n","\n\nconst contentVer = require('../../package.json')['cache-version'].content\nconst hashToSegments = require('../util/hash-to-segments')\nconst path = require('path')\nconst ssri = require('ssri')\n\n// Current format of content file path:\n//\n// sha512-BaSE64Hex= ->\n// ~/.my-cache/content-v2/sha512/ba/da/55deadbeefc0ffee\n//\nmodule.exports = contentPath\nfunction contentPath (cache, integrity) {\n  const sri = ssri.parse(integrity, { single: true })\n  // contentPath is the *strongest* algo given\n  return path.join.apply(path, [\n    contentDir(cache),\n    sri.algorithm\n  ].concat(hashToSegments(sri.hexDigest())))\n}\n\nmodule.exports._contentDir = contentDir\nfunction contentDir (cache) {\n  return path.join(cache, `content-v${contentVer}`)\n}\n","module.exports = {\n  \"_args\": [\n    [\n      \"cacache@12.0.4\",\n      \"C:\\\\Users\\\\87002\\\\Desktop\\\\friendshipclub2022\"\n    ]\n  ],\n  \"_from\": \"cacache@12.0.4\",\n  \"_id\": \"cacache@12.0.4\",\n  \"_inBundle\": false,\n  \"_integrity\": \"sha512-a0tMB40oefvuInr4Cwb3GerbL9xTj1D5yg0T5xrjGCGyfvbxseIXX7BAO/u/hIXdafzOI5JC3wDwHyf24buOAQ==\",\n  \"_location\": \"/cacache\",\n  \"_phantomChildren\": {\n    \"yallist\": \"3.1.1\"\n  },\n  \"_requested\": {\n    \"type\": \"version\",\n    \"registry\": true,\n    \"raw\": \"cacache@12.0.4\",\n    \"name\": \"cacache\",\n    \"escapedName\": \"cacache\",\n    \"rawSpec\": \"12.0.4\",\n    \"saveSpec\": null,\n    \"fetchSpec\": \"12.0.4\"\n  },\n  \"_requiredBy\": [\n    \"/copy-webpack-plugin\",\n    \"/terser-webpack-plugin\"\n  ],\n  \"_resolved\": \"https://registry.npmjs.org/cacache/-/cacache-12.0.4.tgz\",\n  \"_spec\": \"12.0.4\",\n  \"_where\": \"C:\\\\Users\\\\87002\\\\Desktop\\\\friendshipclub2022\",\n  \"author\": {\n    \"name\": \"Kat MarchÃ¡n\",\n    \"email\": \"kzm@sykosomatic.org\"\n  },\n  \"bugs\": {\n    \"url\": \"https://github.com/npm/cacache/issues\"\n  },\n  \"cache-version\": {\n    \"content\": \"2\",\n    \"index\": \"5\"\n  },\n  \"config\": {\n    \"nyc\": {\n      \"exclude\": [\n        \"node_modules/**\",\n        \"test/**\"\n      ]\n    }\n  },\n  \"contributors\": [\n    {\n      \"name\": \"Charlotte Spencer\",\n      \"email\": \"charlottelaspencer@gmail.com\"\n    },\n    {\n      \"name\": \"Rebecca Turner\",\n      \"email\": \"me@re-becca.org\"\n    }\n  ],\n  \"dependencies\": {\n    \"bluebird\": \"^3.5.5\",\n    \"chownr\": \"^1.1.1\",\n    \"figgy-pudding\": \"^3.5.1\",\n    \"glob\": \"^7.1.4\",\n    \"graceful-fs\": \"^4.1.15\",\n    \"infer-owner\": \"^1.0.3\",\n    \"lru-cache\": \"^5.1.1\",\n    \"mississippi\": \"^3.0.0\",\n    \"mkdirp\": \"^0.5.1\",\n    \"move-concurrently\": \"^1.0.1\",\n    \"promise-inflight\": \"^1.0.1\",\n    \"rimraf\": \"^2.6.3\",\n    \"ssri\": \"^6.0.1\",\n    \"unique-filename\": \"^1.1.1\",\n    \"y18n\": \"^4.0.0\"\n  },\n  \"description\": \"Fast, fault-tolerant, cross-platform, disk-based, data-agnostic, content-addressable cache.\",\n  \"devDependencies\": {\n    \"benchmark\": \"^2.1.4\",\n    \"chalk\": \"^2.4.2\",\n    \"cross-env\": \"^5.1.4\",\n    \"require-inject\": \"^1.4.4\",\n    \"standard\": \"^12.0.1\",\n    \"standard-version\": \"^6.0.1\",\n    \"tacks\": \"^1.3.0\",\n    \"tap\": \"^12.7.0\"\n  },\n  \"files\": [\n    \"*.js\",\n    \"lib\",\n    \"locales\"\n  ],\n  \"homepage\": \"https://github.com/npm/cacache#readme\",\n  \"keywords\": [\n    \"cache\",\n    \"caching\",\n    \"content-addressable\",\n    \"sri\",\n    \"sri hash\",\n    \"subresource integrity\",\n    \"cache\",\n    \"storage\",\n    \"store\",\n    \"file store\",\n    \"filesystem\",\n    \"disk cache\",\n    \"disk storage\"\n  ],\n  \"license\": \"ISC\",\n  \"main\": \"index.js\",\n  \"name\": \"cacache\",\n  \"publishConfig\": {\n    \"tag\": \"legacy\"\n  },\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/npm/cacache.git\"\n  },\n  \"scripts\": {\n    \"benchmarks\": \"node test/benchmarks\",\n    \"postrelease\": \"npm publish && git push --follow-tags\",\n    \"prerelease\": \"npm t\",\n    \"pretest\": \"standard\",\n    \"release\": \"standard-version -s\",\n    \"test\": \"cross-env CACACHE_UPDATE_LOCALE_FILES=true tap --coverage --nyc-arg=--all -J test/*.js\",\n    \"test-docker\": \"docker run -it --rm --name pacotest -v \\\"$PWD\\\":/tmp -w /tmp node:latest npm test\"\n  },\n  \"version\": \"12.0.4\"\n}\n","\n\nmodule.exports = hashToSegments\n\nfunction hashToSegments (hash) {\n  return [\n    hash.slice(0, 2),\n    hash.slice(2, 4),\n    hash.slice(4)\n  ]\n}\n","\n\nconst BB = require('bluebird')\n\nconst chownr = BB.promisify(require('chownr'))\nconst mkdirp = BB.promisify(require('mkdirp'))\nconst inflight = require('promise-inflight')\nconst inferOwner = require('infer-owner')\n\n// Memoize getuid()/getgid() calls.\n// patch process.setuid/setgid to invalidate cached value on change\nconst self = { uid: null, gid: null }\nconst getSelf = () => {\n  if (typeof self.uid !== 'number') {\n    self.uid = process.getuid()\n    const setuid = process.setuid\n    process.setuid = (uid) => {\n      self.uid = null\n      process.setuid = setuid\n      return process.setuid(uid)\n    }\n  }\n  if (typeof self.gid !== 'number') {\n    self.gid = process.getgid()\n    const setgid = process.setgid\n    process.setgid = (gid) => {\n      self.gid = null\n      process.setgid = setgid\n      return process.setgid(gid)\n    }\n  }\n}\n\nmodule.exports.chownr = fixOwner\nfunction fixOwner (cache, filepath) {\n  if (!process.getuid) {\n    // This platform doesn't need ownership fixing\n    return BB.resolve()\n  }\n\n  getSelf()\n  if (self.uid !== 0) {\n    // almost certainly can't chown anyway\n    return BB.resolve()\n  }\n\n  return BB.resolve(inferOwner(cache)).then(owner => {\n    const { uid, gid } = owner\n\n    // No need to override if it's already what we used.\n    if (self.uid === uid && self.gid === gid) {\n      return\n    }\n\n    return inflight(\n      'fixOwner: fixing ownership on ' + filepath,\n      () => chownr(\n        filepath,\n        typeof uid === 'number' ? uid : self.uid,\n        typeof gid === 'number' ? gid : self.gid\n      ).catch({ code: 'ENOENT' }, () => null)\n    )\n  })\n}\n\nmodule.exports.chownr.sync = fixOwnerSync\nfunction fixOwnerSync (cache, filepath) {\n  if (!process.getuid) {\n    // This platform doesn't need ownership fixing\n    return\n  }\n  const { uid, gid } = inferOwner.sync(cache)\n  getSelf()\n  if (self.uid === uid && self.gid === gid) {\n    // No need to override if it's already what we used.\n    return\n  }\n  try {\n    chownr.sync(\n      filepath,\n      typeof uid === 'number' ? uid : self.uid,\n      typeof gid === 'number' ? gid : self.gid\n    )\n  } catch (err) {\n    // only catch ENOENT, any other error is a problem.\n    if (err.code === 'ENOENT') {\n      return null\n    }\n    throw err\n  }\n}\n\nmodule.exports.mkdirfix = mkdirfix\nfunction mkdirfix (cache, p, cb) {\n  // we have to infer the owner _before_ making the directory, even though\n  // we aren't going to use the results, since the cache itself might not\n  // exist yet.  If we mkdirp it, then our current uid/gid will be assumed\n  // to be correct if it creates the cache folder in the process.\n  return BB.resolve(inferOwner(cache)).then(() => {\n    return mkdirp(p).then(made => {\n      if (made) {\n        return fixOwner(cache, made).then(() => made)\n      }\n    }).catch({ code: 'EEXIST' }, () => {\n      // There's a race in mkdirp!\n      return fixOwner(cache, p).then(() => null)\n    })\n  })\n}\n\nmodule.exports.mkdirfix.sync = mkdirfixSync\nfunction mkdirfixSync (cache, p) {\n  try {\n    inferOwner.sync(cache)\n    const made = mkdirp.sync(p)\n    if (made) {\n      fixOwnerSync(cache, made)\n      return made\n    }\n  } catch (err) {\n    if (err.code === 'EEXIST') {\n      fixOwnerSync(cache, p)\n      return null\n    } else {\n      throw err\n    }\n  }\n}\n","\n\nconst path = require('path')\nconst y18n = require('y18n')({\n  directory: path.join(__dirname, '../../locales'),\n  locale: 'en',\n  updateFiles: process.env.CACACHE_UPDATE_LOCALE_FILES === 'true'\n})\n\nmodule.exports = yTag\nfunction yTag (parts) {\n  let str = ''\n  parts.forEach((part, i) => {\n    const arg = arguments[i + 1]\n    str += part\n    if (arg) {\n      str += '%s'\n    }\n  })\n  return y18n.__.apply(null, [str].concat([].slice.call(arguments, 1)))\n}\n\nmodule.exports.setLocale = locale => {\n  y18n.setLocale(locale)\n}\n","\n\nconst BB = require('bluebird')\n\nconst figgyPudding = require('figgy-pudding')\nconst fs = require('fs')\nconst index = require('./lib/entry-index')\nconst memo = require('./lib/memoization')\nconst pipe = require('mississippi').pipe\nconst pipeline = require('mississippi').pipeline\nconst read = require('./lib/content/read')\nconst through = require('mississippi').through\n\nconst GetOpts = figgyPudding({\n  integrity: {},\n  memoize: {},\n  size: {}\n})\n\nmodule.exports = function get (cache, key, opts) {\n  return getData(false, cache, key, opts)\n}\nmodule.exports.byDigest = function getByDigest (cache, digest, opts) {\n  return getData(true, cache, digest, opts)\n}\nfunction getData (byDigest, cache, key, opts) {\n  opts = GetOpts(opts)\n  const memoized = (\n    byDigest\n      ? memo.get.byDigest(cache, key, opts)\n      : memo.get(cache, key, opts)\n  )\n  if (memoized && opts.memoize !== false) {\n    return BB.resolve(byDigest ? memoized : {\n      metadata: memoized.entry.metadata,\n      data: memoized.data,\n      integrity: memoized.entry.integrity,\n      size: memoized.entry.size\n    })\n  }\n  return (\n    byDigest ? BB.resolve(null) : index.find(cache, key, opts)\n  ).then(entry => {\n    if (!entry && !byDigest) {\n      throw new index.NotFoundError(cache, key)\n    }\n    return read(cache, byDigest ? key : entry.integrity, {\n      integrity: opts.integrity,\n      size: opts.size\n    }).then(data => byDigest ? data : {\n      metadata: entry.metadata,\n      data: data,\n      size: entry.size,\n      integrity: entry.integrity\n    }).then(res => {\n      if (opts.memoize && byDigest) {\n        memo.put.byDigest(cache, key, res, opts)\n      } else if (opts.memoize) {\n        memo.put(cache, entry, res.data, opts)\n      }\n      return res\n    })\n  })\n}\n\nmodule.exports.sync = function get (cache, key, opts) {\n  return getDataSync(false, cache, key, opts)\n}\nmodule.exports.sync.byDigest = function getByDigest (cache, digest, opts) {\n  return getDataSync(true, cache, digest, opts)\n}\nfunction getDataSync (byDigest, cache, key, opts) {\n  opts = GetOpts(opts)\n  const memoized = (\n    byDigest\n      ? memo.get.byDigest(cache, key, opts)\n      : memo.get(cache, key, opts)\n  )\n  if (memoized && opts.memoize !== false) {\n    return byDigest ? memoized : {\n      metadata: memoized.entry.metadata,\n      data: memoized.data,\n      integrity: memoized.entry.integrity,\n      size: memoized.entry.size\n    }\n  }\n  const entry = !byDigest && index.find.sync(cache, key, opts)\n  if (!entry && !byDigest) {\n    throw new index.NotFoundError(cache, key)\n  }\n  const data = read.sync(\n    cache,\n    byDigest ? key : entry.integrity,\n    {\n      integrity: opts.integrity,\n      size: opts.size\n    }\n  )\n  const res = byDigest\n    ? data\n    : {\n      metadata: entry.metadata,\n      data: data,\n      size: entry.size,\n      integrity: entry.integrity\n    }\n  if (opts.memoize && byDigest) {\n    memo.put.byDigest(cache, key, res, opts)\n  } else if (opts.memoize) {\n    memo.put(cache, entry, res.data, opts)\n  }\n  return res\n}\n\nmodule.exports.stream = getStream\nfunction getStream (cache, key, opts) {\n  opts = GetOpts(opts)\n  let stream = through()\n  const memoized = memo.get(cache, key, opts)\n  if (memoized && opts.memoize !== false) {\n    stream.on('newListener', function (ev, cb) {\n      ev === 'metadata' && cb(memoized.entry.metadata)\n      ev === 'integrity' && cb(memoized.entry.integrity)\n      ev === 'size' && cb(memoized.entry.size)\n    })\n    stream.write(memoized.data, () => stream.end())\n    return stream\n  }\n  index.find(cache, key).then(entry => {\n    if (!entry) {\n      return stream.emit(\n        'error', new index.NotFoundError(cache, key)\n      )\n    }\n    let memoStream\n    if (opts.memoize) {\n      let memoData = []\n      let memoLength = 0\n      memoStream = through((c, en, cb) => {\n        memoData && memoData.push(c)\n        memoLength += c.length\n        cb(null, c, en)\n      }, cb => {\n        memoData && memo.put(cache, entry, Buffer.concat(memoData, memoLength), opts)\n        cb()\n      })\n    } else {\n      memoStream = through()\n    }\n    stream.emit('metadata', entry.metadata)\n    stream.emit('integrity', entry.integrity)\n    stream.emit('size', entry.size)\n    stream.on('newListener', function (ev, cb) {\n      ev === 'metadata' && cb(entry.metadata)\n      ev === 'integrity' && cb(entry.integrity)\n      ev === 'size' && cb(entry.size)\n    })\n    pipe(\n      read.readStream(cache, entry.integrity, opts.concat({\n        size: opts.size == null ? entry.size : opts.size\n      })),\n      memoStream,\n      stream\n    )\n  }).catch(err => stream.emit('error', err))\n  return stream\n}\n\nmodule.exports.stream.byDigest = getStreamDigest\nfunction getStreamDigest (cache, integrity, opts) {\n  opts = GetOpts(opts)\n  const memoized = memo.get.byDigest(cache, integrity, opts)\n  if (memoized && opts.memoize !== false) {\n    const stream = through()\n    stream.write(memoized, () => stream.end())\n    return stream\n  } else {\n    let stream = read.readStream(cache, integrity, opts)\n    if (opts.memoize) {\n      let memoData = []\n      let memoLength = 0\n      const memoStream = through((c, en, cb) => {\n        memoData && memoData.push(c)\n        memoLength += c.length\n        cb(null, c, en)\n      }, cb => {\n        memoData && memo.put.byDigest(\n          cache,\n          integrity,\n          Buffer.concat(memoData, memoLength),\n          opts\n        )\n        cb()\n      })\n      stream = pipeline(stream, memoStream)\n    }\n    return stream\n  }\n}\n\nmodule.exports.info = info\nfunction info (cache, key, opts) {\n  opts = GetOpts(opts)\n  const memoized = memo.get(cache, key, opts)\n  if (memoized && opts.memoize !== false) {\n    return BB.resolve(memoized.entry)\n  } else {\n    return index.find(cache, key)\n  }\n}\n\nmodule.exports.hasContent = read.hasContent\n\nmodule.exports.copy = function cp (cache, key, dest, opts) {\n  return copy(false, cache, key, dest, opts)\n}\nmodule.exports.copy.byDigest = function cpDigest (cache, digest, dest, opts) {\n  return copy(true, cache, digest, dest, opts)\n}\nfunction copy (byDigest, cache, key, dest, opts) {\n  opts = GetOpts(opts)\n  if (read.copy) {\n    return (\n      byDigest ? BB.resolve(null) : index.find(cache, key, opts)\n    ).then(entry => {\n      if (!entry && !byDigest) {\n        throw new index.NotFoundError(cache, key)\n      }\n      return read.copy(\n        cache, byDigest ? key : entry.integrity, dest, opts\n      ).then(() => byDigest ? key : {\n        metadata: entry.metadata,\n        size: entry.size,\n        integrity: entry.integrity\n      })\n    })\n  } else {\n    return getData(byDigest, cache, key, opts).then(res => {\n      return fs.writeFileAsync(dest, byDigest ? res : res.data)\n        .then(() => byDigest ? key : {\n          metadata: res.metadata,\n          size: res.size,\n          integrity: res.integrity\n        })\n    })\n  }\n}\n","\n\nconst LRU = require('lru-cache')\n\nconst MAX_SIZE = 50 * 1024 * 1024 // 50MB\nconst MAX_AGE = 3 * 60 * 1000\n\nlet MEMOIZED = new LRU({\n  max: MAX_SIZE,\n  maxAge: MAX_AGE,\n  length: (entry, key) => {\n    if (key.startsWith('key:')) {\n      return entry.data.length\n    } else if (key.startsWith('digest:')) {\n      return entry.length\n    }\n  }\n})\n\nmodule.exports.clearMemoized = clearMemoized\nfunction clearMemoized () {\n  const old = {}\n  MEMOIZED.forEach((v, k) => {\n    old[k] = v\n  })\n  MEMOIZED.reset()\n  return old\n}\n\nmodule.exports.put = put\nfunction put (cache, entry, data, opts) {\n  pickMem(opts).set(`key:${cache}:${entry.key}`, { entry, data })\n  putDigest(cache, entry.integrity, data, opts)\n}\n\nmodule.exports.put.byDigest = putDigest\nfunction putDigest (cache, integrity, data, opts) {\n  pickMem(opts).set(`digest:${cache}:${integrity}`, data)\n}\n\nmodule.exports.get = get\nfunction get (cache, key, opts) {\n  return pickMem(opts).get(`key:${cache}:${key}`)\n}\n\nmodule.exports.get.byDigest = getDigest\nfunction getDigest (cache, integrity, opts) {\n  return pickMem(opts).get(`digest:${cache}:${integrity}`)\n}\n\nclass ObjProxy {\n  constructor (obj) {\n    this.obj = obj\n  }\n  get (key) { return this.obj[key] }\n  set (key, val) { this.obj[key] = val }\n}\n\nfunction pickMem (opts) {\n  if (!opts || !opts.memoize) {\n    return MEMOIZED\n  } else if (opts.memoize.get && opts.memoize.set) {\n    return opts.memoize\n  } else if (typeof opts.memoize === 'object') {\n    return new ObjProxy(opts.memoize)\n  } else {\n    return MEMOIZED\n  }\n}\n","\n\nconst BB = require('bluebird')\n\nconst contentPath = require('./path')\nconst figgyPudding = require('figgy-pudding')\nconst fs = require('graceful-fs')\nconst PassThrough = require('stream').PassThrough\nconst pipe = BB.promisify(require('mississippi').pipe)\nconst ssri = require('ssri')\nconst Y = require('../util/y.js')\n\nconst lstatAsync = BB.promisify(fs.lstat)\nconst readFileAsync = BB.promisify(fs.readFile)\n\nconst ReadOpts = figgyPudding({\n  size: {}\n})\n\nmodule.exports = read\nfunction read (cache, integrity, opts) {\n  opts = ReadOpts(opts)\n  return withContentSri(cache, integrity, (cpath, sri) => {\n    return readFileAsync(cpath, null).then(data => {\n      if (typeof opts.size === 'number' && opts.size !== data.length) {\n        throw sizeError(opts.size, data.length)\n      } else if (ssri.checkData(data, sri)) {\n        return data\n      } else {\n        throw integrityError(sri, cpath)\n      }\n    })\n  })\n}\n\nmodule.exports.sync = readSync\nfunction readSync (cache, integrity, opts) {\n  opts = ReadOpts(opts)\n  return withContentSriSync(cache, integrity, (cpath, sri) => {\n    const data = fs.readFileSync(cpath)\n    if (typeof opts.size === 'number' && opts.size !== data.length) {\n      throw sizeError(opts.size, data.length)\n    } else if (ssri.checkData(data, sri)) {\n      return data\n    } else {\n      throw integrityError(sri, cpath)\n    }\n  })\n}\n\nmodule.exports.stream = readStream\nmodule.exports.readStream = readStream\nfunction readStream (cache, integrity, opts) {\n  opts = ReadOpts(opts)\n  const stream = new PassThrough()\n  withContentSri(cache, integrity, (cpath, sri) => {\n    return lstatAsync(cpath).then(stat => ({ cpath, sri, stat }))\n  }).then(({ cpath, sri, stat }) => {\n    return pipe(\n      fs.createReadStream(cpath),\n      ssri.integrityStream({\n        integrity: sri,\n        size: opts.size\n      }),\n      stream\n    )\n  }).catch(err => {\n    stream.emit('error', err)\n  })\n  return stream\n}\n\nlet copyFileAsync\nif (fs.copyFile) {\n  module.exports.copy = copy\n  module.exports.copy.sync = copySync\n  copyFileAsync = BB.promisify(fs.copyFile)\n}\n\nfunction copy (cache, integrity, dest, opts) {\n  opts = ReadOpts(opts)\n  return withContentSri(cache, integrity, (cpath, sri) => {\n    return copyFileAsync(cpath, dest)\n  })\n}\n\nfunction copySync (cache, integrity, dest, opts) {\n  opts = ReadOpts(opts)\n  return withContentSriSync(cache, integrity, (cpath, sri) => {\n    return fs.copyFileSync(cpath, dest)\n  })\n}\n\nmodule.exports.hasContent = hasContent\nfunction hasContent (cache, integrity) {\n  if (!integrity) { return BB.resolve(false) }\n  return withContentSri(cache, integrity, (cpath, sri) => {\n    return lstatAsync(cpath).then(stat => ({ size: stat.size, sri, stat }))\n  }).catch(err => {\n    if (err.code === 'ENOENT') { return false }\n    if (err.code === 'EPERM') {\n      if (process.platform !== 'win32') {\n        throw err\n      } else {\n        return false\n      }\n    }\n  })\n}\n\nmodule.exports.hasContent.sync = hasContentSync\nfunction hasContentSync (cache, integrity) {\n  if (!integrity) { return false }\n  return withContentSriSync(cache, integrity, (cpath, sri) => {\n    try {\n      const stat = fs.lstatSync(cpath)\n      return { size: stat.size, sri, stat }\n    } catch (err) {\n      if (err.code === 'ENOENT') { return false }\n      if (err.code === 'EPERM') {\n        if (process.platform !== 'win32') {\n          throw err\n        } else {\n          return false\n        }\n      }\n    }\n  })\n}\n\nfunction withContentSri (cache, integrity, fn) {\n  return BB.try(() => {\n    const sri = ssri.parse(integrity)\n    // If `integrity` has multiple entries, pick the first digest\n    // with available local data.\n    const algo = sri.pickAlgorithm()\n    const digests = sri[algo]\n    if (digests.length <= 1) {\n      const cpath = contentPath(cache, digests[0])\n      return fn(cpath, digests[0])\n    } else {\n      return BB.any(sri[sri.pickAlgorithm()].map(meta => {\n        return withContentSri(cache, meta, fn)\n      }, { concurrency: 1 }))\n        .catch(err => {\n          if ([].some.call(err, e => e.code === 'ENOENT')) {\n            throw Object.assign(\n              new Error('No matching content found for ' + sri.toString()),\n              { code: 'ENOENT' }\n            )\n          } else {\n            throw err[0]\n          }\n        })\n    }\n  })\n}\n\nfunction withContentSriSync (cache, integrity, fn) {\n  const sri = ssri.parse(integrity)\n  // If `integrity` has multiple entries, pick the first digest\n  // with available local data.\n  const algo = sri.pickAlgorithm()\n  const digests = sri[algo]\n  if (digests.length <= 1) {\n    const cpath = contentPath(cache, digests[0])\n    return fn(cpath, digests[0])\n  } else {\n    let lastErr = null\n    for (const meta of sri[sri.pickAlgorithm()]) {\n      try {\n        return withContentSriSync(cache, meta, fn)\n      } catch (err) {\n        lastErr = err\n      }\n    }\n    if (lastErr) { throw lastErr }\n  }\n}\n\nfunction sizeError (expected, found) {\n  var err = new Error(Y`Bad data size: expected inserted data to be ${expected} bytes, but got ${found} instead`)\n  err.expected = expected\n  err.found = found\n  err.code = 'EBADSIZE'\n  return err\n}\n\nfunction integrityError (sri, path) {\n  var err = new Error(Y`Integrity verification failed for ${sri} (${path})`)\n  err.code = 'EINTEGRITY'\n  err.sri = sri\n  err.path = path\n  return err\n}\n","\n\nconst figgyPudding = require('figgy-pudding')\nconst index = require('./lib/entry-index')\nconst memo = require('./lib/memoization')\nconst write = require('./lib/content/write')\nconst to = require('mississippi').to\n\nconst PutOpts = figgyPudding({\n  algorithms: {\n    default: ['sha512']\n  },\n  integrity: {},\n  memoize: {},\n  metadata: {},\n  pickAlgorithm: {},\n  size: {},\n  tmpPrefix: {},\n  single: {},\n  sep: {},\n  error: {},\n  strict: {}\n})\n\nmodule.exports = putData\nfunction putData (cache, key, data, opts) {\n  opts = PutOpts(opts)\n  return write(cache, data, opts).then(res => {\n    return index.insert(\n      cache, key, res.integrity, opts.concat({ size: res.size })\n    ).then(entry => {\n      if (opts.memoize) {\n        memo.put(cache, entry, data, opts)\n      }\n      return res.integrity\n    })\n  })\n}\n\nmodule.exports.stream = putStream\nfunction putStream (cache, key, opts) {\n  opts = PutOpts(opts)\n  let integrity\n  let size\n  const contentStream = write.stream(\n    cache, opts\n  ).on('integrity', int => {\n    integrity = int\n  }).on('size', s => {\n    size = s\n  })\n  let memoData\n  let memoTotal = 0\n  const stream = to((chunk, enc, cb) => {\n    contentStream.write(chunk, enc, () => {\n      if (opts.memoize) {\n        if (!memoData) { memoData = [] }\n        memoData.push(chunk)\n        memoTotal += chunk.length\n      }\n      cb()\n    })\n  }, cb => {\n    contentStream.end(() => {\n      index.insert(cache, key, integrity, opts.concat({ size })).then(entry => {\n        if (opts.memoize) {\n          memo.put(cache, entry, Buffer.concat(memoData, memoTotal), opts)\n        }\n        stream.emit('integrity', integrity)\n        cb()\n      })\n    })\n  })\n  let erred = false\n  stream.once('error', err => {\n    if (erred) { return }\n    erred = true\n    contentStream.emit('error', err)\n  })\n  contentStream.once('error', err => {\n    if (erred) { return }\n    erred = true\n    stream.emit('error', err)\n  })\n  return stream\n}\n","\n\nconst BB = require('bluebird')\n\nconst contentPath = require('./path')\nconst fixOwner = require('../util/fix-owner')\nconst fs = require('graceful-fs')\nconst moveFile = require('../util/move-file')\nconst PassThrough = require('stream').PassThrough\nconst path = require('path')\nconst pipe = BB.promisify(require('mississippi').pipe)\nconst rimraf = BB.promisify(require('rimraf'))\nconst ssri = require('ssri')\nconst to = require('mississippi').to\nconst uniqueFilename = require('unique-filename')\nconst Y = require('../util/y.js')\n\nconst writeFileAsync = BB.promisify(fs.writeFile)\n\nmodule.exports = write\nfunction write (cache, data, opts) {\n  opts = opts || {}\n  if (opts.algorithms && opts.algorithms.length > 1) {\n    throw new Error(\n      Y`opts.algorithms only supports a single algorithm for now`\n    )\n  }\n  if (typeof opts.size === 'number' && data.length !== opts.size) {\n    return BB.reject(sizeError(opts.size, data.length))\n  }\n  const sri = ssri.fromData(data, {\n    algorithms: opts.algorithms\n  })\n  if (opts.integrity && !ssri.checkData(data, opts.integrity, opts)) {\n    return BB.reject(checksumError(opts.integrity, sri))\n  }\n  return BB.using(makeTmp(cache, opts), tmp => (\n    writeFileAsync(\n      tmp.target, data, { flag: 'wx' }\n    ).then(() => (\n      moveToDestination(tmp, cache, sri, opts)\n    ))\n  )).then(() => ({ integrity: sri, size: data.length }))\n}\n\nmodule.exports.stream = writeStream\nfunction writeStream (cache, opts) {\n  opts = opts || {}\n  const inputStream = new PassThrough()\n  let inputErr = false\n  function errCheck () {\n    if (inputErr) { throw inputErr }\n  }\n\n  let allDone\n  const ret = to((c, n, cb) => {\n    if (!allDone) {\n      allDone = handleContent(inputStream, cache, opts, errCheck)\n    }\n    inputStream.write(c, n, cb)\n  }, cb => {\n    inputStream.end(() => {\n      if (!allDone) {\n        const e = new Error(Y`Cache input stream was empty`)\n        e.code = 'ENODATA'\n        return ret.emit('error', e)\n      }\n      allDone.then(res => {\n        res.integrity && ret.emit('integrity', res.integrity)\n        res.size !== null && ret.emit('size', res.size)\n        cb()\n      }, e => {\n        ret.emit('error', e)\n      })\n    })\n  })\n  ret.once('error', e => {\n    inputErr = e\n  })\n  return ret\n}\n\nfunction handleContent (inputStream, cache, opts, errCheck) {\n  return BB.using(makeTmp(cache, opts), tmp => {\n    errCheck()\n    return pipeToTmp(\n      inputStream, cache, tmp.target, opts, errCheck\n    ).then(res => {\n      return moveToDestination(\n        tmp, cache, res.integrity, opts, errCheck\n      ).then(() => res)\n    })\n  })\n}\n\nfunction pipeToTmp (inputStream, cache, tmpTarget, opts, errCheck) {\n  return BB.resolve().then(() => {\n    let integrity\n    let size\n    const hashStream = ssri.integrityStream({\n      integrity: opts.integrity,\n      algorithms: opts.algorithms,\n      size: opts.size\n    }).on('integrity', s => {\n      integrity = s\n    }).on('size', s => {\n      size = s\n    })\n    const outStream = fs.createWriteStream(tmpTarget, {\n      flags: 'wx'\n    })\n    errCheck()\n    return pipe(inputStream, hashStream, outStream).then(() => {\n      return { integrity, size }\n    }).catch(err => {\n      return rimraf(tmpTarget).then(() => { throw err })\n    })\n  })\n}\n\nfunction makeTmp (cache, opts) {\n  const tmpTarget = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix)\n  return fixOwner.mkdirfix(\n    cache, path.dirname(tmpTarget)\n  ).then(() => ({\n    target: tmpTarget,\n    moved: false\n  })).disposer(tmp => (!tmp.moved && rimraf(tmp.target)))\n}\n\nfunction moveToDestination (tmp, cache, sri, opts, errCheck) {\n  errCheck && errCheck()\n  const destination = contentPath(cache, sri)\n  const destDir = path.dirname(destination)\n\n  return fixOwner.mkdirfix(\n    cache, destDir\n  ).then(() => {\n    errCheck && errCheck()\n    return moveFile(tmp.target, destination)\n  }).then(() => {\n    errCheck && errCheck()\n    tmp.moved = true\n    return fixOwner.chownr(cache, destination)\n  })\n}\n\nfunction sizeError (expected, found) {\n  var err = new Error(Y`Bad data size: expected inserted data to be ${expected} bytes, but got ${found} instead`)\n  err.expected = expected\n  err.found = found\n  err.code = 'EBADSIZE'\n  return err\n}\n\nfunction checksumError (expected, found) {\n  var err = new Error(Y`Integrity check failed:\n  Wanted: ${expected}\n   Found: ${found}`)\n  err.code = 'EINTEGRITY'\n  err.expected = expected\n  err.found = found\n  return err\n}\n","\n\nconst fs = require('graceful-fs')\nconst BB = require('bluebird')\nconst chmod = BB.promisify(fs.chmod)\nconst unlink = BB.promisify(fs.unlink)\nlet move\nlet pinflight\n\nmodule.exports = moveFile\nfunction moveFile (src, dest) {\n  // This isn't quite an fs.rename -- the assumption is that\n  // if `dest` already exists, and we get certain errors while\n  // trying to move it, we should just not bother.\n  //\n  // In the case of cache corruption, users will receive an\n  // EINTEGRITY error elsewhere, and can remove the offending\n  // content their own way.\n  //\n  // Note that, as the name suggests, this strictly only supports file moves.\n  return BB.fromNode(cb => {\n    fs.link(src, dest, err => {\n      if (err) {\n        if (err.code === 'EEXIST' || err.code === 'EBUSY') {\n          // file already exists, so whatever\n        } else if (err.code === 'EPERM' && process.platform === 'win32') {\n          // file handle stayed open even past graceful-fs limits\n        } else {\n          return cb(err)\n        }\n      }\n      return cb()\n    })\n  }).then(() => {\n    // content should never change for any reason, so make it read-only\n    return BB.join(unlink(src), process.platform !== 'win32' && chmod(dest, '0444'))\n  }).catch(() => {\n    if (!pinflight) { pinflight = require('promise-inflight') }\n    return pinflight('cacache-move-file:' + dest, () => {\n      return BB.promisify(fs.stat)(dest).catch(err => {\n        if (err.code !== 'ENOENT') {\n          // Something else is wrong here. Bail bail bail\n          throw err\n        }\n        // file doesn't already exist! let's try a rename -> copy fallback\n        if (!move) { move = require('move-concurrently') }\n        return move(src, dest, { BB, fs })\n      })\n    })\n  })\n}\n","\n\nconst BB = require('bluebird')\n\nconst index = require('./lib/entry-index')\nconst memo = require('./lib/memoization')\nconst path = require('path')\nconst rimraf = BB.promisify(require('rimraf'))\nconst rmContent = require('./lib/content/rm')\n\nmodule.exports = entry\nmodule.exports.entry = entry\nfunction entry (cache, key) {\n  memo.clearMemoized()\n  return index.delete(cache, key)\n}\n\nmodule.exports.content = content\nfunction content (cache, integrity) {\n  memo.clearMemoized()\n  return rmContent(cache, integrity)\n}\n\nmodule.exports.all = all\nfunction all (cache) {\n  memo.clearMemoized()\n  return rimraf(path.join(cache, '*(content-*|index-*)'))\n}\n","\n\nconst BB = require('bluebird')\n\nconst contentPath = require('./path')\nconst hasContent = require('./read').hasContent\nconst rimraf = BB.promisify(require('rimraf'))\n\nmodule.exports = rm\nfunction rm (cache, integrity) {\n  return hasContent(cache, integrity).then(content => {\n    if (content) {\n      const sri = content.sri\n      if (sri) {\n        return rimraf(contentPath(cache, sri)).then(() => true)\n      }\n    } else {\n      return false\n    }\n  })\n}\n","\n\nmodule.exports = require('./lib/verify')\n","\n\nconst BB = require('bluebird')\n\nconst contentPath = require('./content/path')\nconst figgyPudding = require('figgy-pudding')\nconst finished = BB.promisify(require('mississippi').finished)\nconst fixOwner = require('./util/fix-owner')\nconst fs = require('graceful-fs')\nconst glob = BB.promisify(require('glob'))\nconst index = require('./entry-index')\nconst path = require('path')\nconst rimraf = BB.promisify(require('rimraf'))\nconst ssri = require('ssri')\n\nBB.promisifyAll(fs)\n\nconst VerifyOpts = figgyPudding({\n  concurrency: {\n    default: 20\n  },\n  filter: {},\n  log: {\n    default: { silly () {} }\n  }\n})\n\nmodule.exports = verify\nfunction verify (cache, opts) {\n  opts = VerifyOpts(opts)\n  opts.log.silly('verify', 'verifying cache at', cache)\n  return BB.reduce([\n    markStartTime,\n    fixPerms,\n    garbageCollect,\n    rebuildIndex,\n    cleanTmp,\n    writeVerifile,\n    markEndTime\n  ], (stats, step, i) => {\n    const label = step.name || `step #${i}`\n    const start = new Date()\n    return BB.resolve(step(cache, opts)).then(s => {\n      s && Object.keys(s).forEach(k => {\n        stats[k] = s[k]\n      })\n      const end = new Date()\n      if (!stats.runTime) { stats.runTime = {} }\n      stats.runTime[label] = end - start\n      return stats\n    })\n  }, {}).tap(stats => {\n    stats.runTime.total = stats.endTime - stats.startTime\n    opts.log.silly('verify', 'verification finished for', cache, 'in', `${stats.runTime.total}ms`)\n  })\n}\n\nfunction markStartTime (cache, opts) {\n  return { startTime: new Date() }\n}\n\nfunction markEndTime (cache, opts) {\n  return { endTime: new Date() }\n}\n\nfunction fixPerms (cache, opts) {\n  opts.log.silly('verify', 'fixing cache permissions')\n  return fixOwner.mkdirfix(cache, cache).then(() => {\n    // TODO - fix file permissions too\n    return fixOwner.chownr(cache, cache)\n  }).then(() => null)\n}\n\n// Implements a naive mark-and-sweep tracing garbage collector.\n//\n// The algorithm is basically as follows:\n// 1. Read (and filter) all index entries (\"pointers\")\n// 2. Mark each integrity value as \"live\"\n// 3. Read entire filesystem tree in `content-vX/` dir\n// 4. If content is live, verify its checksum and delete it if it fails\n// 5. If content is not marked as live, rimraf it.\n//\nfunction garbageCollect (cache, opts) {\n  opts.log.silly('verify', 'garbage collecting content')\n  const indexStream = index.lsStream(cache)\n  const liveContent = new Set()\n  indexStream.on('data', entry => {\n    if (opts.filter && !opts.filter(entry)) { return }\n    liveContent.add(entry.integrity.toString())\n  })\n  return finished(indexStream).then(() => {\n    const contentDir = contentPath._contentDir(cache)\n    return glob(path.join(contentDir, '**'), {\n      follow: false,\n      nodir: true,\n      nosort: true\n    }).then(files => {\n      return BB.resolve({\n        verifiedContent: 0,\n        reclaimedCount: 0,\n        reclaimedSize: 0,\n        badContentCount: 0,\n        keptSize: 0\n      }).tap((stats) => BB.map(files, (f) => {\n        const split = f.split(/[/\\\\]/)\n        const digest = split.slice(split.length - 3).join('')\n        const algo = split[split.length - 4]\n        const integrity = ssri.fromHex(digest, algo)\n        if (liveContent.has(integrity.toString())) {\n          return verifyContent(f, integrity).then(info => {\n            if (!info.valid) {\n              stats.reclaimedCount++\n              stats.badContentCount++\n              stats.reclaimedSize += info.size\n            } else {\n              stats.verifiedContent++\n              stats.keptSize += info.size\n            }\n            return stats\n          })\n        } else {\n          // No entries refer to this content. We can delete.\n          stats.reclaimedCount++\n          return fs.statAsync(f).then(s => {\n            return rimraf(f).then(() => {\n              stats.reclaimedSize += s.size\n              return stats\n            })\n          })\n        }\n      }, { concurrency: opts.concurrency }))\n    })\n  })\n}\n\nfunction verifyContent (filepath, sri) {\n  return fs.statAsync(filepath).then(stat => {\n    const contentInfo = {\n      size: stat.size,\n      valid: true\n    }\n    return ssri.checkStream(\n      fs.createReadStream(filepath),\n      sri\n    ).catch(err => {\n      if (err.code !== 'EINTEGRITY') { throw err }\n      return rimraf(filepath).then(() => {\n        contentInfo.valid = false\n      })\n    }).then(() => contentInfo)\n  }).catch({ code: 'ENOENT' }, () => ({ size: 0, valid: false }))\n}\n\nfunction rebuildIndex (cache, opts) {\n  opts.log.silly('verify', 'rebuilding index')\n  return index.ls(cache).then(entries => {\n    const stats = {\n      missingContent: 0,\n      rejectedEntries: 0,\n      totalEntries: 0\n    }\n    const buckets = {}\n    for (let k in entries) {\n      if (entries.hasOwnProperty(k)) {\n        const hashed = index._hashKey(k)\n        const entry = entries[k]\n        const excluded = opts.filter && !opts.filter(entry)\n        excluded && stats.rejectedEntries++\n        if (buckets[hashed] && !excluded) {\n          buckets[hashed].push(entry)\n        } else if (buckets[hashed] && excluded) {\n          // skip\n        } else if (excluded) {\n          buckets[hashed] = []\n          buckets[hashed]._path = index._bucketPath(cache, k)\n        } else {\n          buckets[hashed] = [entry]\n          buckets[hashed]._path = index._bucketPath(cache, k)\n        }\n      }\n    }\n    return BB.map(Object.keys(buckets), key => {\n      return rebuildBucket(cache, buckets[key], stats, opts)\n    }, { concurrency: opts.concurrency }).then(() => stats)\n  })\n}\n\nfunction rebuildBucket (cache, bucket, stats, opts) {\n  return fs.truncateAsync(bucket._path).then(() => {\n    // This needs to be serialized because cacache explicitly\n    // lets very racy bucket conflicts clobber each other.\n    return BB.mapSeries(bucket, entry => {\n      const content = contentPath(cache, entry.integrity)\n      return fs.statAsync(content).then(() => {\n        return index.insert(cache, entry.key, entry.integrity, {\n          metadata: entry.metadata,\n          size: entry.size\n        }).then(() => { stats.totalEntries++ })\n      }).catch({ code: 'ENOENT' }, () => {\n        stats.rejectedEntries++\n        stats.missingContent++\n      })\n    })\n  })\n}\n\nfunction cleanTmp (cache, opts) {\n  opts.log.silly('verify', 'cleaning tmp directory')\n  return rimraf(path.join(cache, 'tmp'))\n}\n\nfunction writeVerifile (cache, opts) {\n  const verifile = path.join(cache, '_lastverified')\n  opts.log.silly('verify', 'writing verifile to ' + verifile)\n  try {\n    return fs.writeFileAsync(verifile, '' + (+(new Date())))\n  } finally {\n    fixOwner.chownr.sync(cache, verifile)\n  }\n}\n\nmodule.exports.lastRun = lastRun\nfunction lastRun (cache) {\n  return fs.readFileAsync(\n    path.join(cache, '_lastverified'), 'utf8'\n  ).then(data => new Date(+data))\n}\n","\n\nconst BB = require('bluebird')\n\nconst figgyPudding = require('figgy-pudding')\nconst fixOwner = require('./fix-owner')\nconst path = require('path')\nconst rimraf = BB.promisify(require('rimraf'))\nconst uniqueFilename = require('unique-filename')\n\nconst TmpOpts = figgyPudding({\n  tmpPrefix: {}\n})\n\nmodule.exports.mkdir = mktmpdir\nfunction mktmpdir (cache, opts) {\n  opts = TmpOpts(opts)\n  const tmpTarget = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix)\n  return fixOwner.mkdirfix(cache, tmpTarget).then(() => {\n    return tmpTarget\n  })\n}\n\nmodule.exports.withTmp = withTmp\nfunction withTmp (cache, opts, cb) {\n  if (!cb) {\n    cb = opts\n    opts = null\n  }\n  opts = TmpOpts(opts)\n  return BB.using(mktmpdir(cache, opts).disposer(rimraf), cb)\n}\n\nmodule.exports.fix = fixtmpdir\nfunction fixtmpdir (cache) {\n  return fixOwner(cache, path.join(cache, 'tmp'))\n}\n"]}